# PyRL
Environment Agnostic RL algorithm implementations using Pytorch.

Code is type-hinted & uses minibatches - a common downfall to public libraries

Currently 4 DQN-style algorithims implemented - see exmples.ipynb for one particular environment tested

1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  
2. *DQN Experience Replay*  <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub> 
3. *DQN with Fixed targets* <sub><sup>([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub> 
4. *Double Q Learning (DDQN)* <sub><sup> ([arXiv:1509.06461v3 [cs.LG] 8 Dec 2015](https://arxiv.org/pdf/1509.06461v3.pdf)) </sup></sub>   

 --- IN PROGRESS ---
1. *DQN Prioritized Replay* <sub><sup> ([Google DeepMind et al. 2016](https://arxiv.org/pdf/1511.05952v3.pdf)) </sup></sub>   
2. REINFORCE
3. PPO
4. A2C
5. A3C