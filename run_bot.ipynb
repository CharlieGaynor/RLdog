{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b61e407-cf36-46b5-9930-d0f3979b7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b286780f-c38f-4642-91a3-22fd0afab425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class runner():\n",
    "\n",
    "    def __init__(self, net, env, num_updates=100000, num_obs=104,\n",
    "                 lr=5e-4, epsilon=1, min_epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net (eg_model class): eg_model class, describing a neural network\n",
    "            env (Trumps env): Custom enviroment made (swap for gym envs for example)\n",
    "            num_updates (int, optional): Number of games to train for. Defaults to 100000.\n",
    "            num_obs (int, optional): State space. Defaults to 104.\n",
    "            lr (float, optional): Learning rate. Defaults to 5e-4.\n",
    "            epsilon (int, optional): Initial exploration rate. Defaults to 1.\n",
    "            min_epsilon (float, optional): Min exploration rate. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # constants\n",
    "        self.num_obs = num_obs\n",
    "        self.lr = lr\n",
    "        self.num_updates = num_updates\n",
    "        self.best_loss = np.inf\n",
    "        self.sum_rewards = []\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.ep_multiplier = self.min_epsilon**(1 / num_updates)\n",
    "        self.scores = []\n",
    "        # Using gpu? Set False for now\n",
    "        self.is_cuda = False\n",
    "\n",
    "        \"\"\"Environment\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "        \"\"\"Network\"\"\"\n",
    "        self.net = net\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        \"\"\"Play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "        for episode in range(1, self.num_updates + 1):\n",
    "            total_reward = 0\n",
    "            s, pc = self.env.reset()\n",
    "\n",
    "            # Plays a game\n",
    "            for t in range(t_max):\n",
    "                a = self.net.get_action(s, epsilon=self.epsilon)\n",
    "                next_s, r, done, pc = self.env.step(a)\n",
    "\n",
    "                if train:  # Train on what just happened\n",
    "                    self.net.opt.zero_grad()\n",
    "                    loss = self.compute_td_loss(\n",
    "                        [s], [a], [r], [next_s], [done])\n",
    "                    loss.backward()\n",
    "                    self.net.opt.step()\n",
    "\n",
    "                total_reward += r\n",
    "                s = next_s\n",
    "                if done:\n",
    "                    self.sum_rewards.append(total_reward)\n",
    "                    self.epsilon = self.epsilon * self.ep_multiplier\n",
    "                    break\n",
    "\n",
    "            # Print Stuff\n",
    "            if (episode) % 20000 == 0:\n",
    "                print(f\"{episode}\")\n",
    "            if (episode) % 5000 == 0:\n",
    "                self.evaluate()\n",
    "            if (episode) % 400 == 0:\n",
    "                print('.', end='')\n",
    " \n",
    "        \n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates model performance with epsilon = 0 for 500 games\n",
    "        \"\"\"        \n",
    "        \n",
    "        for _ in range(500):\n",
    "            total_reward = 0\n",
    "            s, pc = self.env.reset()\n",
    "            for __ in range(100):\n",
    "                a = self.net.get_action(s, epsilon=0)\n",
    "                next_s, r, done, pc = self.env.step(a)\n",
    "                total_reward += r\n",
    "                s = next_s\n",
    "                if done:\n",
    "                    self.scores.append(total_reward)\n",
    "                    break\n",
    "        return\n",
    "\n",
    "    def compute_td_loss(\n",
    "            self,\n",
    "            states,\n",
    "            actions,\n",
    "            rewards,\n",
    "            next_states,\n",
    "            is_done,\n",
    "            gamma=0.99):\n",
    "        \"\"\"Compute loss function according to bellman equation\n",
    "\n",
    "        Args:\n",
    "            states (np.array): History of states from the last game\n",
    "            actions (np.array): History of actions from the last game\n",
    "            rewards (np.array): History of rewards from the last game\n",
    "            next_states (np.array): History of states shifted by 1, from the last game\n",
    "            is_done (bool): History of whether the game was finished, from the last game\n",
    "            gamma (float, optional): discount rate. Defaults to 0.99.\n",
    "\n",
    "        Returns:\n",
    "            [torch.tensor] : Loss function ready to back propagate\n",
    "        \"\"\" \n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "        is_done = torch.tensor(is_done, dtype=torch.uint8) \n",
    "\n",
    "        predicted_qvalues = self.net.forward(states)\n",
    "\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
    "\n",
    "        predicted_next_qvalues = self.net.forward(next_states)\n",
    "\n",
    "        max_next_state_value = torch.max(predicted_next_qvalues, dim=1)[0]\n",
    "\n",
    "        target_qvalues_for_actions = rewards + torch.mul(gamma, max_next_state_value)\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a)\n",
    "        # since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done==1, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions) ** 2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Saves model\"\"\"        \n",
    "        with open('Scripts/models/epsilon_greedy_1_model', 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb6e1e-5667-4932-89cc-52fa6a6c552c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
